# -*- coding: utf-8 -*-
"""CIS9650.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iraPuZ_dOmHNsQlkiiMaKcms73No0dF3
"""

import seaborn as sns
import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
!pip install dmba
from dmba import classificationSummary, gainsChart, liftChart
from sklearn.ensemble import RandomForestClassifier

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

dating = pd.read_csv("speed-dating2_V3.csv")

dating.shape # the shape of the data

dating.columns # the columns of the dataset

# histograms of the age distribution based on the gender and on the matching status

import matplotlib.pyplot as plt
g = sns.FacetGrid(dating, hue="gender",  row="match", height = 5, aspect = 3, palette = {'female':"g",'male':"r"}, legend_out=True, despine = False, margin_titles = True)
g.map_dataframe(sns.histplot, x="age")

g = (g.set_axis_labels("Age","Age Distribution").
set(xlim=(16,40),ylim=(0,600)))
g.set_titles("Age")
plt.legend(title='Color Legend', loc='upper left', labels=['Female', 'Male'])

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# visual that shows the relationship between funny and intelligence with the distribution for these variables
import matplotlib.pyplot as plt
# %matplotlib inline
sns.set_theme(style="white")
g = sns.JointGrid(data=dating, x="intelligence", y="funny", height = 10, ratio = 7,xlim=(6, 10), ylim=(6, 10), space = 1, dropna =True)
g.plot_joint(sns.kdeplot,
             fill=True)
g.plot_marginals(sns.histplot, color="y", alpha=1, bins=55 , weights = 5)
plt.show()

dating.field = dating.field.astype('category') # convert field to a categorical variable

dating.field.value_counts() # get the frequency of each profession

import numpy as np
dating.frequent_professions = dating.field.replace(dating.field.value_counts().index[7:], np.nan) # get the most 7 frequent professions and create a new column

dating.frequent_professions

# violinplot that shows the distribution of age for the most frequent professions
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(30, 10))

# Draw a violinplot with a narrower bandwidth than the default
sns.violinplot(y = dating.age,x=dating.frequent_professions, palette="muted", bw=.2, cut=1, linewidth=1)

# Finalize the figure
ax.set(ylim=(18, 50))
ax.set(xlim=(None, None))
sns.despine(left=True, bottom=True)

# heatmap of the ages of the persons who took the survey and their dates
import numpy as np
table = pd.pivot_table(dating, values='match', index="age",
                    columns="age_o", aggfunc=np.sum)

# Draw a heatmap with the numeric values in each cell
f, ax = plt.subplots(figsize=(25, 10))
sns.heatmap(table, annot=True, vmin = 0, vmax = 15,  linewidths=.5, ax=ax, center =1, fmt = ".1f",cmap="YlGnBu")

dating.dropna(inplace=True) # drop the rows with at least one empty value

# define the predictors and the outcome of the machine learning algorithms
predictors = ['age', 'age_o', 'samerace',
       'pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence',
       'pref_o_funny', 'pref_o_ambitious',
       'pref_o_shared_interests', 'attractive_o',
       'intelligence_o', 'funny_o',
       'shared_interests_o', 'sports', 'exercise',
       'dining', 'museums',
        'art', 'hiking',
       'gaming', 'clubbing', 'reading', 'tv']
outcome = 'match'

X = dating[predictors]
y = dating[outcome]
X.shape

train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.3, random_state=1) 
# divide the dataset into training and test

from sklearn.tree import DecisionTreeClassifier
from dmba import plotDecisionTree

"""Logistic Regression"""

# apply the logistic regression 
logit_reg = LogisticRegression(penalty="l2", C=1e42, solver='liblinear',class_weight = 'balanced')
logit_reg.fit(train_X, train_y)

print(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X.columns)) # the weight of each attribute

# print the metrics of the model
from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score,roc_auc_score,classification_report, confusion_matrix
lr_prediction_train = logit_reg.predict_proba(train_X)[:,1] > 0.5
lr_prediction_valid = logit_reg.predict_proba(valid_X)[:,1] > 0.5
print("Accuracy on train is:",accuracy_score(train_y,lr_prediction_train))
print("Accuracy on test is:",accuracy_score(valid_y,lr_prediction_valid))
print("Precision_score train is:",precision_score(train_y,lr_prediction_train))
print("Precision_score on test is:",precision_score(valid_y,lr_prediction_valid))
print("Recall_score on train is:",recall_score(train_y,lr_prediction_train))
print("Recall_score on test is:",recall_score(valid_y,lr_prediction_valid))
print("f1_score on train is:",f1_score(train_y,lr_prediction_train))
print("f1_score on test is:",f1_score(valid_y,lr_prediction_valid))

"""
Decision Tree
"""

# apply the decision tree model
DecisionTree = DecisionTreeClassifier(max_depth = 4)
DecisionTree.fit(train_X, train_y)

plotDecisionTree(DecisionTree, feature_names=train_X.columns)

importances = DecisionTree.feature_importances_

im = pd.DataFrame({'feature': train_X.columns, 'importance': importances})
im = im.sort_values('importance',ascending=False)
print(im)

dt_prediction_train = DecisionTree.predict(train_X) 
dt_prediction_valid = DecisionTree.predict(valid_X) 
    
print("Accuracy score on train is:",accuracy_score(train_y,dt_prediction_train))
print("Accuracy score on test is:",accuracy_score(valid_y,dt_prediction_valid))
print("Precision score on train is:",precision_score(train_y,dt_prediction_train))
print("Precision score on test is:",precision_score(valid_y,dt_prediction_valid))
print("Recall score on train is:",recall_score(train_y,dt_prediction_train))
print("Recall score on test is:",recall_score(valid_y,dt_prediction_valid))
print("F1 score on train is:",f1_score(train_y,dt_prediction_train))
print("F1 score on test is:",f1_score(valid_y,dt_prediction_valid))

"""Gradient Boosted Tree"""

from sklearn.ensemble import GradientBoostingClassifier

gbm = GradientBoostingClassifier(random_state=0)
gbm.fit(train_X, train_y)
gbm.predict(valid_X[:2])

importances = list(zip(gbm.feature_importances_, valid_X.columns))
pd.DataFrame(importances, index=[x for (_,x) in importances]).sort_values(by = 0, ascending = False).plot(kind = 'bar', color = 'r')

gbt_prediction_train = gbm.predict(train_X)
gbt_prediction_valid = gbm.predict(valid_X)

print("Accuracy on train is:",accuracy_score(train_y,gbt_prediction_train))
print("Accuracy on test is:",accuracy_score(valid_y,gbt_prediction_valid))
print("Precision_score train is:",precision_score(train_y,gbt_prediction_train))
print("Precision_score on test is:",precision_score(valid_y,gbt_prediction_valid))
print("Recall_score on train is:",recall_score(train_y,gbt_prediction_train))
print("Recall_score on test is:",recall_score(valid_y,gbt_prediction_valid))
print("f1_score on train is:",f1_score(train_y,gbt_prediction_train))
print("f1_score on test is:",f1_score(valid_y,gbt_prediction_valid))

